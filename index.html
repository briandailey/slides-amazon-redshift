<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">

        <title>Amazon Redshift: What You Need To Know (PyTennessee 2014)</title>

        <meta name="description" content="Amazon Redshift: The basics, what you need to know to get started, and some caveats.">
        <meta name="author" content="Brian Dailey">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <link rel="stylesheet" href="css/reveal.min.css">
        <link rel="stylesheet" href="css/theme/default.css" id="theme">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">

        <!-- If the query includes 'print-pdf', use the PDF print sheet -->
        <script>
            document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->
    </head>

    <body>

        <div class="reveal">

            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">
                <section data-background="img/stars.jpg">
                    <h1>Amazon Redshift</h1>
                    <h3>What You Need To Know</h3>
                    <p>
                    <small>
                        PyTN 2014<br/>
                        <a href="http://www.flickr.com/photos/nasamarshall/5095567894/in/photostream/">Photo: NASA</a>
                    </small>
                    </p>
                </section>

                <section>
                    <h3>Who Is This Guy?</h3>
                    <h4>Brian Dailey</h4>
                    <p>CTO &amp; Co-founder at Stratasan</p>
                    <p>
                        Monkeys around a bit with Python, PostgreSQL,<br/>
                        Django, and a few more bits and pieces.
                    </p><p>
                        <a href="http://twitter.com/byeliad">@byeliad</a>
                    </p>
                    <aside class="notes">A little about me, get this out of the way quickly. Stratasan does market research, takes in data and gives a very "bird's-eye" view.</aside>
                </section>

                <section>
                    <h3>toc</h3>
                    <ul>
                        <li>What is Redshift?</li>
                        <li>Why We Use It</li>
                        <li>Cost Overview</li>
                        <li>Rough Benchmarks</li>
                        <li>Redshift Architecture</li>
                    </ul>
                    <aside class="notes">I always like to tell you what I'm going to tell you first.</aside>
                </section>

                <section>
                    <ul>
                        <li>Building Tables</li>
                        <li>Distribution and Sort Keys</li>
                        <li>Loading Data</li>
                        <li>Querying Data</li>
                        <li>Extracting Data</li>
                        <li>Optimization Tips</li>
                        <li>Web UI</li>
                    </ul>
                </section>

                <!-- Example of nested vertical slides -->
                <section>
                    <h2>What's Redshift?</h2>
                    <aside class="notes">I'm going to assume you have <em>some</em> idea of what RDBMSs are and how they work.</aside>
                </section>
                <section>
                    <h3>bizspeak:</h3>
                    <p><q cite="https://aws.amazon.com/redshift/">
                        Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service ...
                    </q></p>
                    <aside class="notes">This is the description precisely from Amazon. But it's not really written for you and me.</aside>
                </section>
                <section>
                    <img width="320" height="299" src="img/haha-business.jpg" alt="Ha ha business!">
                    <aside class="notes">Because, business!</aside>
                </section>
                <section>
                    <p>"It achieves its high performance through extreme parallelism, columnar data storage, and smart data compression." - <a href="http://nerds.airbnb.com/redshift-performance-cost">AirBnB</a></p>
                    <aside class="notes">This gets down to the brass tacks.
                     Before Amazon released Redshift, a company called ParAccel had built a managed database
                     platform that was designed for analytics, touted to run on commodity hardware.
                     Who has a lot of commodity hardware? Amazon made a significant investment in the company,
                     and used their platform to create Redshift.</aside>
                </section>
                <section>
                    <h3>Why we use it</h3>
                    <ul>
                        <li>It looks a lot like PostgreSQL</li>
                        <li>Much cheaper than analytic appliances (Vertica et al)</li>
                        <li>Easy and fast ad-hoc aggregate queries</li>
                    </ul>
                    <aside class="notes">
                        The learning curve was short for us to get started.
                        Vertica was fairly expensive, longer learning curve. See the AirBNB writeup.
                        Since it uses columnar storage, sums, avgs, counts are quite fast.
                    </aside>
                </section>
                <section>
                    <h3>If this is PostgreSQL:</h3>
                    <img src="img/edward-500.jpg" alt="normal">
                    <aside class="notes">If PostgreSQL is Robert Pattison, then Redshift...</aside>
                </section>
                <section>
                    <h3>This is Redshift:</h3>
                    <div class="vertical-align: middle;">
                    <img src="img/edward-wax.png" alt="wax" style="display: inline; vertical-align: middle;" align="middle">
                    +
                    <img src="img/formula_1-300.jpg" alt="F1" style="display: inline; vertical-align: middle;" align="middle">
                    <aside class="notes">Is a wax, not-quite-real uncanny valley copy of Robert fused with an F1 racer.
                        It looks and smells like PostgreSQL 8.0.2 but there are quite a few subtle differences once you scratch the surface.
                    </aside>
                </div>
                </section>

                <section>
                    <h3>What's it Cost?</h3>
                    <aside class="notes">
                        I mentioned it was cheaper than Vertica, but here are ballpark numbers.
                    </aside>
                </section>

                <section style="font-size: 80%;">
                    <table><colgroup><col style="width: 16%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"></colgroup><caption>US East (N. Virginia)</caption><thead><tr><th></th><th class="rate">vCPU</th><th class="rate">ECU</th><th class="rate">Memory (GiB)</th><th class="rate">Storage</th><th class="rate">I/O</th><th class="rate">Price</th></tr></thead><tbody class="body"><tr class="heading"><th colspan="7" class="type">DW1 - Dense Storage</th></tr><tr class="tiers"><td class="size">dw1.xlarge</td><td class="vCPU">2</td><td class="ECU">4.4</td><td class="memory">15</td><td class="storage">2TB HDD</td><td class="io">0.30GB/s</td><td class="rate yrTerm1Hourly">$0.850 per Hour</td></tr><tr class="tiers"><td class="size">dw1.8xlarge</td><td class="vCPU">16</td><td class="ECU">35</td><td class="memory">120</td><td class="storage">16TB HDD</td><td class="io">2.40GB/s</td><td class="rate yrTerm1Hourly">$6.800 per Hour</td></tr></tbody><tbody class="body"><tr class="heading"><th colspan="7" class="type">DW2 - Dense Compute</th></tr><tr class="tiers"><td class="size">dw2.large</td><td class="vCPU">2</td><td class="ECU">7</td><td class="memory">15</td><td class="storage">0.16TB SSD</td><td class="io">0.20GB/s</td><td class="rate yrTerm1Hourly">$0.250 per Hour</td></tr><tr class="tiers"><td class="size">dw2.8xlarge</td><td class="vCPU">32</td><td class="ECU">104</td><td class="memory">244</td><td class="storage">2.56TB SSD</td><td class="io">3.70GB/s</td><td class="rate yrTerm1Hourly">$4.800 per Hour</td></tr></tbody></table>
                    <aside class="notes">
                        There are some differences in cost based on whether you are
                        doing dense storage, or dense compute. SSD (DW2) is compute.
                        Higher cost per TB, but lower cost per node.
                    </aside>
                </section>

                <section style="font-size: 80%;">
                    <h3>Per Terabyte</h3>
                    <table width="100%"><caption>US East (N. Virginia)</caption><thead><tr><th></th><th colspan="3" class="yrTerm">Effective Price per TB per Year</th></tr><tr><th></th><th class="type">On-Demand</th><th class="type">1yr RI</th><th class="type">3yr RI</th></tr></thead><tbody class="body"><tr class="tiers"><td class="tier">dw1.xlarge</td><td class="price OD">$3,723 </td><td class="price 1yRI">$2,192 </td><td class="price 3yRI">$999 </td></tr><tr class="tiers"><td class="tier">dw1.8xlarge</td><td class="price OD">$3,723 </td><td class="price 1yRI">$2,192 </td><td class="price 3yRI">$999 </td></tr><tr class="tiers"><td class="tier">dw2.large</td><td class="price OD">$13,688 </td><td class="price 1yRI">$8,794 </td><td class="price 3yRI">$5,498 </td></tr><tr class="tiers"><td class="tier">dw2.8xlarge</td><td class="price OD">$16,425 </td><td class="price 1yRI">$11,018 </td><td class="price 3yRI">$5,498 </td></tr></tbody></table>
                    <aside class="notes">
                        Here you can see the per terabyte cost a little more
                        clearly. Obviously, the SSD setup (which I'll talk more
                        about shortly) is higher per TB.
                    </aside>
                </section>

                <section>
                    <h1>Speed</h1>
                    <aside class="notes">
                        The $64k question. How fast is it? Now in previous
                        versions of this talk I've ran my own stats, but I
                        don't do a great job of that so I'm going to leave it
                        to some fellows at Berkley.
                    </aside>
                </section>

                <section>
                    <h3>Scan Query</h3>
                    <img src="img/scan_query_speed.png" alt="Scan Query Speed" />
                    <aside class="notes">
                        Redshift is the blue bar on the far left.
                        A is a small result set.
                        B is intermediate.
                        C is ETL-like (spread across several nodes).
                    </aside>
                </section>

                <section>
                    <h3>Aggregate Query</h3>
                    <img src="img/aggregate_query_speed.png" alt="Aggregate Query Speed" />
                    <aside class="notes">
                        "Redshift's columnar storage provides greater benefit than in [scan query] since several columns of the UserVistits table are un-used."
                    </aside>
                </section>

                <section>
                    <h3>Join Query</h3>
                    <img src="img/join_query_speed.png" alt="Join Query Speed" />
                    <aside class="notes">
                        "Redshift has an edge in this case because the overall network capacity in the cluster is higher."
                    </aside>
                </section>

                <section>
                    <blockquote>"Hive, Impala, and Shark are used is because they offer a high degree of flexibility, both in terms of the underlying format of the data and the type of computation employed."</blockquote>
                    <p><a href="https://amplab.cs.berkeley.edu/benchmark/">Full Study: https://amplab.cs.berkeley.edu/benchmark/</a></p>
                    <aside class="notes">
                        The authors point out that Hive et al are more flexible. Also, they are open source. You can
                        run them on your own hardware. Can't do that with Redshift (yet).
                    </aside>
                </section>

                <section>
                    <p>AirBnB tested against Hadoop...</p>
                    <aside class="notes">
                        AirBnB did some early testing and published their findings.
                    </aside>
                </section>

                <section>
                    <blockquote>"Simple range query against a giant table with 3 billion rows, we saw 5x performance improvement over Hive!"</blockquote>
                    <aside class="notes">Emphasis not added by me.</aside>
                </section>


                <section>
                    <h1>What It's Good For</h1>
                </section>
                <section><h3>Analytics</h3><aside class="notes">Particularly aggregate queries. Because it's columnar, sums, avgs, etc are particularly fast.</aside></section>
                <section><h1>... And not good for</h1></section>
                <section><h3>Your web application</h3><aside class="notes">If you're doing a lot of SELECT FROM WHERE ID = then you are gonna have a bad time. It doesn't use indices, so single lookups are dog slow.</aside></section>

                <section>
                <h1>Redshift<br/>
                    Architecture</h1>
                </section>

                <section>
                    <a href="http://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html"><img src="img/redshift-architecture.png" alt="redshift architecture" /></a>
                    <aside class="notes">The Redshift cluster in it's natural environment consists of a leader node and multiple compute nodes.
                        Each node in the cluster has its own operating system, dedicated memory, and dedicated disk storage.</aside>
                </section>

                <section data-background="img/composer.png">
                    <h1>Leader Node</h1>
                    <p><small><a href="http://www.flickr.com/photos/frederikmagle/6880669820/">Photo: Frederik Magle</a></small></p>
                    <aside class="notes">
                        The leader node manages the distribution of data and query processing tasks to the compute nodes.
                        Only created if there are 2 or more nodes. This is what you're talking to and interacting with
                        via SQL. It plans the queries, distributes it to the compute nodes, and aggregates the results.
                        It is where all of the pg_ (catalog) tables are found.
                        If SQL doesn't hit data (e.g., SELECT NOW()) it's run only on the leader node.
                    </aside>
                </section>
                <section data-background="img/symphony.jpg">
                    <h1>Compute Nodes</h1>
                    <p><small><a href="http://www.flickr.com/photos/paulrobinsonuk/8406097564/">Photo: Paul Robinson</a></small></p>
                    <aside class="notes">Does the processing and hands it back to the leader node. Grunts of the Redshift world.</aside>
                </section>

                <section>
                    <h3>Types of nodes...</h3>
                    <ol>
                        <li>XL: 2 cores, 15GiB memory, 3 disk drives with 2TB of local attached storage.</li>
                        <li>8XL: 16 cores, 120GiB memory, 24 disk drives (16TB)</li>
                        <li><b>(New)</b> SSD XL:  - 160 GB of SSD storage, 2 Intel Xeon E5-2670v2 virtual cores, and 15 GiB of RAM.</li>
                        <li><b>(New)</b> SSD 8XL:  - 2.56 TB of SSD storage, 32 Intel Xeon E5-2670v2 virtual cores, and 244 GiB of RAM.</li>
                    </ol>
                    <aside class="notes">
                        This goes back to the dense storage vs. dense compute.
                        The SSD options were introduced last month. They look awesome.
                        I've not had a chance to try them yet, but early reports
                        say they are blazing fast (of course).
                    </aside>
                </section>

                <section>
                <ul>
                    <li>Each node has multiple slices (one for each core)</li>
                    <li>Executes queries in parallel over each slice</li>
                    <li>Data is divvied up to each slice by a distribution key...</li>
                </ul>
                    <aside class="notes">
                        The data on a compute node is distributed across slices.
                        Slices correspond to the nodes processors, the idea is
                        that they can be scanned in parallel.
                        Data is managed so that it is spread evenly across all
                        slices. You can customize this a bit to optimize table
                        scans and joins. We'll cover that shortly.
                    </aside>
                </section>

                <section>
                    <h2>Building Tables</h2>
                    <aside class="notes">
                        Before you do anything on Redshift you'll need to build your
                        tables! What options are different than PostgreSQL?</aside>
                </section>

                <section data-background="img/anarchy.jpg">
                    <h1>unenforced<br/>constraints</h1>
                    <p><a href="http://www.flickr.com/photos/80497449@N04/7383943916/">Photo: Nicolas Raymand</a></p>
                </section>
                <section data-background="img/anarchy.jpg">
                    <p><q cite="http://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-defining-constraints.html">“Amazon Redshift does not enforce unique, primary-key, and foreign-key constraints.”</q></p>
                    <aside class="notes">
                        Anarchy is unleashed! Foreign keys and primary keys
                        must be enforced by your application. That said, you can
                        still tell Redshift where they are used and it will
                        use them to plan and optimize queries.
                    </aside>
                </section>
                <section>
                    <h3>unsupported types</h3>
                    <ul>
                        <li>Arrays</li>
                        <li>INTERVAL, TIME, TIMESTAMP WITH TIMEZONE</li>
                        <li>SERIAL, MONEY</li>
                        <li>XML</li>
                        <li>UUID</li>
                        <li><a href="http://docs.aws.amazon.com/redshift/latest/dg/c_unsupported-postgresql-datatypes.html">...and more...</a></li>
                    </ul>
                    <aside class="notes">
                        Secondly, if you’re migrating from an existing database,
                        you’ll notice that there are a lot of unsupported
                        PostgreSQL data types. The ones that hurt the most for
                        us was SERIAL, since we used that for primary key
                        columns.
                    </aside>
                </section>
                <section>
                    <h3>supported types</h3>
                    <ul>
                        <li>SMALLINT</li>
                        <li>INTEGER</li>
                        <li>BIGINT</li>
                        <li>DECIMAL</li>
                        <li>REAL</li>
                        <li>DOUBLE PRECISION</li>
                        <li>BOOLEAN</li>
                        <li>CHAR</li>
                        <li>VARCHAR</li>
                        <li>DATE</li>
                        <li>TIMESTAMP</li>
                        <li><b>(New)</b> JSON</li>
                    </ul>
                    <aside class="notes">
                        It's almost easier to enumerate over
                        what they DO support. Note that JSON support was added
                        in September 2013. It stores it inside VARCHAR, much
                        like PostgreSQL does.
                    </aside>
                </section>
                <section>
                    <img src="img/dragons-small.jpg" alt="Dragons" />
                    <aside class="notes">
                        Here are some dragons that bit us.
                    </aside>
                </section>
                <section>
                    <blockquote cite="http://docs.aws.amazon.com/redshift/latest/dg/r_Character_types.html#r_Character_types-char-or-character">
                        "CHAR and VARCHAR data types are defined<br/>
                        in terms of bytes, not characters."</blockquote>
                    <aside class="notes">
                        When you issue a CREATE statement note that length
                        on these types is in bytes, not characters. If you do
                        anything with multibyte data, this will silently
                        truncate your strings. Terrible to debug.
                        UTF8 up to 4 bytes is supported.
                    </aside>
                </section>

                <section>
                    <h1>Distributing Data</h1>
                </section>

                <section>
                    <h3>Distribution Key</h3>
                    <ul>
                        <li>Default is round robin / even distribution.</li>
                        <li>Like values can be (approximately) stored together
                        on node slices.</li>
                        <li>ALL distribution copies the table to each node.</li>
                        <li>Aim is to distribute data evenly across slices.</li>
                    </ul>
                    <aside class="notes">
                        EVEN is the default. Follows a round-robin distribution.
                        Use this if table is denormalized, doesn't
                        participate in joins, or doesn't have a clear choice on
                        dist key.
                        KEY give you one column and attempts to group like
                        values together on slices. If you join on a column
                        frequently, use this.
                        "ALL distribution is appropriate only for relatively slow
                        moving tables; that is, tables that are not updated
                        frequently or extensively. Small dimension tables do not
                        benefit significantly from ALL distribution, because the
                        cost of redistribution is low."
                    </aside>
                </section>
                <section>
                    <p>In general, if you use KEY you still want to aim for even
                    distribution!</p>
                    <aside class="notes">
                        An example of this would be a table with an
                        "favorite_color" column. If the values are pretty
                        evently distributed, then grouping them together means
                        Redshift has to do less work in copying data around
                        inside the cluster to perform the aggregate. On the
                        other hand, if 90% of your table data has a
                        favorite_color of "red", then the node or slice with the
                        "red" data is going to have to do a disproportionate
                        amount of work.
                    </aside>
                </section>
                <section>
                    <h3>Sort Key</h3>
                    <blockquote cite="http://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html">Amazon Redshift stores your data on disk in sorted order according to the sort key.</blockquote>
                </section>
                <section>
                    <blockquote>“Amazon Redshift stores columnar data in 1 MB
                    disk blocks. The min and max values for each block are
                    stored as part of the metadata.”</blockquote>
                    <aside class="notes">
                        This means that if you have a sort key indicated, Redshift
                        can skip over swaths of your data to pinpoint exactly where the
                        value you’re looking for can be found.
                        Amazon’s webinars often use the example of a “last
                        updated timestamp” column to demonstrate how this works.
                        If you are most commonly accessing data based on that
                        timestamp you can use it as a sort key. When searching
                        for values, it will only look at nodes, slices, and
                        blocks that are relevant.
                    </aside>
                </section>
                <section data-background="img/jobs.jpg">
                    <h2>One more thing...</h2>
                    <ul>
                        <li>You can't ALTER COLUMN.</li>
                        <li>Only one ADD COLUMN per ALTER TABLE.</li>
                    </ul>
                    <br/>
                    <br/>
                    <p><small>Photo credit: <a href="http://www.flickr.com/photos/acaben/541420967/" title="Steve Jobs Keynote by acaben, on Flickr">Ben Stanfield</a></small></p>
                    <aside class="notes">
                        If you have to modify a column, you'll have to add one,
                        copy the data over, and rename it.
                        If you need to add multiple columns it may be easier to
                        just create the new table and copy everything over into
                        it.
                    </aside>
                </section>

                <section>
                    <h1>Loading Data Into Redshift</h1>
                    <aside class="notes">
                        Now that you've created your table structure,
                        let's talk about how to get data into it.</aside>
                </section>

                <section>
                    <h2>Your Options</h2>
                    <ul>
                        <li>S3</li>
                        <li><b>(New)</b> Remote Host (via SSH)</li>
                        <li>DynamoDB</li>
                    </ul>
                    <aside class="notes">
                        The remote host option was added in November 2013.
                    </aside>
                </section>

                <section>
                    <h3>Best Practice</h3>
                    <h1>S3</h1>
                    <h4>(Per Amazon)</h4>
                    <aside class="notes">
                        I'm going to spend the most time detailing how to get
                        your data uploaded via S3, since that's what Amazon
                        recommends as a best practice. It's also the most
                        straightforward way to get data into Redshift.
                    </aside>
                </section>

                <section>
                    <h3>s3cmd is your friend.</h3>
                    <p><a href="http://s3tools.org/s3cmd">http://s3tools.org/s3cmd</a></p>
                    <pre><code>s3cmd put gigantic_csv_file.csv.part.gz.00000 s3://mybucket/mypath/</code></pre>
                    <aside class="notes">
                        You can also use boto, which would allow you to stream
                        the upload of multiple files in parallel. s3cmd doesn't
                        yet do that.
                        You can gzip your files before uploading to save
                        bandwidth. You can also separate your file into chunks
                        and Redshift will load them in parallel.
                    </aside>
                </section>

                <section>
<pre><code style="word-wrap: break-word;">COPY table_name [ (column1 [,column2, ...]) ]
FROM 's3://objectpath'
[ WITH ] CREDENTIALS [AS] 'aws_access_credentials'
[ option [ ... ] ]</code></pre>
                    <aside class="notes">
                        Here is the basic COPY syntax. You give the bucket and
                        path to the file, and your AWS credentials so Redshift
                        can load it. Note that you really want to do any data
                        transformations before you load it into Redshift in
                        bulk.
                    </aside>
                </section>

                <section>
                    <ul>
                        <li><i>objectpath</i> is a prefix. Loads in parallel.</li>
                        <li>gzip</li>
                        <li>encryption</li>
                    </ul>
                    <aside class="notes">
                        Again, note that the object path is a prefix. If you
                        have chunked files, you can load in parallel. COPY has
                        several other options, too. You can load it gzipped, or
                        load an encrypted file.
                    </aside>
                </section>

                <section>
                    <h3>Loading via Remote Host</h3>
                    <p>Connect to instances via SSH, executes commands, and loads generated text output.</p> 
                    <aside class="notes">
                        Uses the clusters public key to connect. Create a JSON
                        manifest file on S3 that tells Redshift what commands
                        to run.
                    </aside>
                </section>

                <section>
                    <h2>Debugging</h2>
                    <ul>
                        <li>STL_LOAD_ERRORS</li>
                        <li>STL_LOADERROR_DETAIL</li>
                        <li>STL_S3CLIENT</li>
                    </ul>
                    <aside class="notes">yo
                        So how do you track down issues when a COPY goes wrong?
                        These tables are your log file equivalents. They will
                        tell you what went wrong.
                        STL_LOAD_ERRORS gives overview, DETAIL gives row
                        specific information, and S3CLIENT tells you about any
                        errors connecting to S3.
                    </aside>
                </section>

                <section>
                    <h2>Debugging</h2>
                    <ul>
                        <li>NOLOAD</li>
                        <li>MAXERROR</li>
                    </ul>
                    <aside class="notes">
                        It's a good idea to do a dry run with NOLOAD as you can
                        snuff out any issues quickly. By default, REdshift will
                        stop loading after the first error, so you can set
                        MAXERROR to 0 to see all errors. Just don't forget to
                        reset that when you are finished testing!
                    </aside>
                </section>

                <section>
                    <img src="img/dragons-small.jpg" alt="Dragons" />
                    <aside class="notes">
                        Here are some more gotchas we ran into when loading data
                        into Redshift.
                    </aside>
                </section>

                <section>
                    <p>You need 2.5x data size to load if sorted or to vacuum table.</p>
                </section>

                <section>
                    <p>Date or timestamp columns must be formatted in same way (only defined once).</p>
                    <p>(ACCEPTANYDATE will load NULL when format does not match.)</p>
                    <aside class="notes">
                        ACCEPTANYDATE will effectively set errors to null,
                        rather than puking and stopping the load. If you have
                        varying time formats that you want to load into a
                        datestamp, you will need to do some transformation prior
                        to loading.
                    </aside>
                </section>

                <section>
                    <p>Ingest via SQL</p>
                    <p>Sure...</p>
                    <aside class="notes">
                        You <em>could</em>, but it's going to be slow.</aside>
                </section>

                <section>
                    <img src="img/wtf-is-wrong-with-you.jpg" alt="WTF is wrong with you?" />
                </section>


                <section>
                    <h1>QUERYING DATA</h1>
                    <aside class="notes">
                        So we've defined our structure and we've loaded some
                        data. What's next? Queries!
                    </aside>
                </section>

                <section><h3>Looks/smells like Postgres 8.0.2.</h3>
                    <aside class="notes">
                       We're not going to spend a lot of time here because, by
                       golly, if you've used Postgres much you should be in
                       familiar territory.</aside>
                </section>

                <section>
                    <ul>
                        <li>AVG(INT) will return INT.</li>
                        <li>Note that this is NOT listed in "Features implemented differently."</li>
                    </ul>
                    <aside class="notes">
                        However, this is one thing I ran into.
                        Postgres 8 will return a decimal, but Redshift returns
                        an int truncated.
                    </aside>
                </section>

                <section>
                    <h1>EXTRACTING DATA</h1>
                    <aside class="notes">
                        How do we get our data out of Redshift?</aside>
                </section>

                <section>
<pre><code style="word-wrap: break-word;">unload ('select * from tablename')
to 's3://mybucket/'
credentials 'aws_access_key_id=[your-access-key-id];
aws_secret_access_key=[your-secret-access-key]';
</code></pre>
                    <aside class="notes">
                        It's pretty easy. Same as COPY, but in reverse. Push the
                        data back to Amazon S3 for backups or to copy down
                        locally. UNLOAD has many options, you can compress or
                        encrypt the data, or specify format.</aside>
                </section>

                <section>
                    <p>UNLOAD dumps to multiple files</p>
                    <pre><code>s3://bucket/table_data_0000
s3://bucket/table_data_0000</code></pre>
                    <aside class="notes">
                        One little gotcha is that UNLOAD creates a file for each
                        slice. So multiple notes are going to mean multiple
                        files. This is true even if it's a relatively small
                        query (e.g., "SELECT COUNT()").
                    </aside>
                </section>

                <section>
                    <p>...unless you trick it.</p>
                    <pre><code>unload ('select * from (
select * from ... where ...) limit 2147483647') ...</code></pre>
                    <aside class="notes">
                        There is one way around this, but it's not terribly
                        elegant. That LIMIT is the max unsigned integer.
                        If it's over that, you are SOL.
                    </aside>
                </section>

                <section>
                    <p>But if you're over 5.8GiB, files will still split.</p>
                    <p>It's also slower.</p>
                    <aside class="notes">
                        Because it has to aggregate all of that and it can't do
                        the UNLOAD in parallel, it'll be slower. It also runs
                        into the S3 file size limit, which will cause the file
                        to break into parts.
                    </aside>
                </section>

                <section>
                    <h1>Optimization</h1>
                    <aside class="notes">
                        Redshift, like any database platform, can be optimized.
                        Here are a few tricks that you should know
                        about.</aside>
                </section>

                <section>
                    <h4>Workload Manager (WLM)</h4>
                    <p>Controls concurrent queries. Less means more memory per process.</p>
                    <p>"By default, a cluster is configured with one queue that can run five queries concurrently."</p>
                    <!-- http://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html -->
                    <aside class="notes">
                        The WLM is kind of a multi-lane queuing mechanism for queries. You
                        can segment long running queries into one lane, and
                        shorter ones into another higher-priority lane.

                        You can create up to 8 queues, and you can allow up to 15
                        concurrent queries. One strategy is to set the
                        concurrent queries to 1 when you are running a COPY
                        statement. That speeds up the load at the cost of making
                        other queries wait. You can also assign queries at
                        runtime, or set them up by user.
                    </aside>
                </section>

                <section>
                    <h3>VACUUM (ANALYZE)</h3>
                    <p>Vacuum early, vacuum often.</p>
                    <p>Anytime you've made non-trivial changes to data.</p>
                </section>

                <section>
                    <p>VACUUM is expensive. Schedule for slow days.</p>
                </section>

                <section>
                    <p>Only one VACUUM at a time per cluster.</p>
                </section>

                <section>
                    <p>VACUUM doesn't resort unaffected slices.</p>
                </section>

                <section>
                    <p>Debugging VACUUM</p>
                    <ul>
                        <li>svv_vacuum_progress</li>
                        <li>svv_vacuum_summary</li>
                    </ul>
                </section>

                <section>
                    <p>Size your columns appropriately.</p>
                </section>

                <section>
                    <p>DATE or TIMESTAMP > CHAR</p>
                </section>

                <section><p>Test different configurations!</p>
                    <ul>
                        <li>2 DW1 8XL nodes?</li>
                        <li>16 DW1 XL nodes?</li>
                        <li>5 DW2 XL nodes?</li>
                    </ul>
                </section>

                <section><h2>Web UI</h2></section>
                <section><img src="img/redshift_create_cluster_1.png" alt="create cluster" /></section>
                <section><img src="img/redshift_create_cluster_2.png" alt="create cluster" /></section>
                <section><img src="img/redshift_create_cluster_3.png" alt="create cluster" /></section>
                <section><img src="img/redshift_performance_console.png" alt="Performance console" /></section>
                <section><img src="img/redshift_queries.png" alt="Queries" /></section>
                <section><img src="img/redshift_query_detail.png" alt="Query Detail" /></section>
                <section><img src="img/redshift_copies.png" alt="Copying" /></section>
                <section><img src="img/redshift_copy_detail.png" alt="Copy detail" /></section>

                <section><h1>Wrapping up</h1></section>

                <section><h2>Redshift is cost-effective.</h2></section>
                <section><h2>...fast.</h2></section>
                <section><h2>...familiar.</h2></section>

                <section>
                    <h1>Redshift</h1>
                    <h3>Nice balance!</h3>
                </section>

                <section>
                <p>"In fact, our analysts like Redshift so much that they don’t want to go back to Hive and other tools even though a few key features are lacking in Redshift. Also, we have noticed that big joins of billions of rows tend to run for a very long time, so for that we’d go back to hadoop for help." -- <a href="http://nerds.airbnb.com/redshift-performance-cost">AirBnB</a></p>
                </section>

                <section>
                <p>"Oddly enough, Redshift isn’t going to sell because devs think it’s super-duper-whizz-bang. It’s going to sell because it took a problem and an industry famous for it’s opaque pricing, high TCO, and unreliable results and completely turned it on its head." -- <a href="http://blog.aggregateknowledge.com/2013/05/16/aws-redshift-how-amazon-changed-the-game/">AK Tech Block</a></p>
                </section>

                <section>
                    <h1>THE END</h1>
                    <h2>Thanks!</h2>
                    <h3>Questions? Tomatoes?</h3>
                    <p>
                    <a href="http://www.twitter.com/byeliad">@byeliad</a><br/>
                    <a href="http://github.com/briandailey">github.com/briandailey</a><br/>
                    </p>
                    <p>Thanks to <a href="http://www.twitter.com/jasonamyers">@jasonamyers</a> for suffering through my first draft.</p>
                </section>

            </div>

        </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.min.js"></script>

        <script>

            // Full list of configuration options available here:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: true,

                theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
                transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

                // Optional libraries used to extend on reveal.js
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
                ]
            });

        </script>

    </body>
</html>
